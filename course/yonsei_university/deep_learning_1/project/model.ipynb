{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb431ef-b6f6-4244-92f3-707ffc279fc5",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Convolutional AutoEncoder (CAE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ca69cc-0f0d-451a-92d5-663e696a70f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1310b3-ae91-428c-904e-aa62326b9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cce51-e4f3-49eb-a834-6809674087cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b43fd99-6b70-40da-97bd-98be9c3971d1",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f7a5f-9558-4501-b248-ebbaccd9efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_gpu(gpu_id: int=0) -> None:\n",
    "    device = torch.device(f'cuda:{gpu_id}')\n",
    "    torch.cuda.set_device(device)\n",
    "    curr_gpu = torch.cuda.current_device()\n",
    "    print(f'Assigned [device:GPU:{curr_gpu}]')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c073d5-9b97-45c8-8031-a3fa8e784949",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 2\n",
    "device = assign_gpu(gpu_id=gpu_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5631791-f9fb-4dfb-a76c-af207ae75c67",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91069dbc-17c0-45c2-b52d-9fce28093b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdTransform:\n",
    "    def __init__(self, thr_255):\n",
    "        self.thr = thr_255 / 255.\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return (x > self.thr).to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e37290-5381-4a99-b4ab-2b8607788da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform:\n",
    "    \n",
    "    def __init__(self, target_size: int, crop_size: int, bw_thresh: int) -> None:\n",
    "        self.target_size = target_size\n",
    "        self.crop_size = crop_size\n",
    "        self.bw_thresh = bw_thresh\n",
    "        self.transform_output = self._transform()\n",
    "    \n",
    "    def _transform(self):\n",
    "        # Without any data augmentation\n",
    "        transform = {\n",
    "            'train': T.Compose([\n",
    "                T.Resize((self.target_size, self.target_size)),\n",
    "                T.CenterCrop(self.crop_size),\n",
    "                T.Grayscale(),\n",
    "                T.ToTensor(),\n",
    "                ThresholdTransform(thr_255=self.bw_thresh),\n",
    "            ]),\n",
    "            'test': T.Compose([\n",
    "                T.Resize((self.target_size, self.target_size)),\n",
    "                T.CenterCrop(self.crop_size),\n",
    "                T.Grayscale(),\n",
    "                T.ToTensor(),\n",
    "                ThresholdTransform(thr_255=self.bw_thresh),\n",
    "            ]),\n",
    "        }\n",
    "        return transform\n",
    "    \n",
    "    def __call__(self, img: Image, split: str):\n",
    "        return self.transform_output[split](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503b69d-c940-4811-9e22-1669e958f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    label = ['normal', 'abnormal']\n",
    "    \n",
    "    def __init__(self, path, split: str='train',\n",
    "                 extension: str='png', transform=None) -> None:\n",
    "        self.path = path\n",
    "        self.split = split\n",
    "        self.extension = extension\n",
    "        self.transform = transform\n",
    "        self.image_path = []\n",
    "        self.label_list = []\n",
    "        \n",
    "        self._get_image_path()\n",
    "    \n",
    "    def _get_image_path(self) -> None:\n",
    "        if self.split == 'test':\n",
    "            for class_ in CustomDataset.label:\n",
    "                self._get_split_dir(class_)\n",
    "        else:\n",
    "            class_ = 'normal'\n",
    "            self._get_split_dir(class_)\n",
    "    \n",
    "    def _get_split_dir(self, class_: str) -> None:\n",
    "        label = 0 if class_ == 'normal' else 1\n",
    "        img_dir_path = f'{self.path}/{class_}/*.{self.extension}'\n",
    "        full_image_path = glob(img_dir_path)\n",
    "        label_list = [label] * len(full_image_path)\n",
    "        self.image_path += full_image_path\n",
    "        self.label_list += label_list\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    def __getitem__(self ,idx) -> tuple[Image, str]:\n",
    "        file_path = self.image_path[idx]\n",
    "        label = self.label_list[idx]\n",
    "        img = Image.open(file_path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img, self.split)\n",
    "        return img, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ec12f21-c54a-4741-82cb-fda1de93ddb7",
   "metadata": {},
   "source": [
    "### - Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35455cd8-27e9-4683-b588-72f1094d83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = 224\n",
    "crop_size = 32\n",
    "bw_thresh = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfd8ed-5467-4d2a-b3e8-6f26b49ba0e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = CustomTransform(resize, crop_size, bw_thresh)\n",
    "train_set = CustomDataset(path='dataset/train', split='train', transform=transform)\n",
    "test_set = CustomDataset(path='dataset/test', split='test', transform=transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d7dba35-15d2-4ce3-8728-9a0f1383c278",
   "metadata": {},
   "source": [
    "### - DataLoader Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517102c-c465-4f7e-baf8-6bbb24795e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a93924-e972-4c0d-a844-2d488a56f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f197fe92-c4ca-4809-b4ba-e419aa103b14",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "\n",
    "- 국소패턴 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aaba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, type_: str, *args, **kwargs) -> None:\n",
    "        super(Pooling, self).__init__()\n",
    "        self.type_ = type_.upper()\n",
    "        self.max_pooling = nn.MaxPool2d(*args, **kwargs)\n",
    "        self.avg_pooling = nn.AvgPool2d(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        if self.type_.startswith('MAX'):\n",
    "            x = self.max_pooling(x)\n",
    "        else:\n",
    "            x = self.avg_pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in: int, c_out: int, *args, **kwargs) -> None:\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, *args, **kwargs),\n",
    "            nn.ReLU(),\n",
    "            Pooling('max', kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in: int, c_out: int, last_layer: bool=False, *args, **kwargs) -> None:\n",
    "        super(TransConvBlock, self).__init__()\n",
    "        self.last_layer = last_layer\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_in, c_out, *args, **kwargs),\n",
    "        )\n",
    "        self.output = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        if self.last_layer:\n",
    "            return self.block(x)\n",
    "        else:\n",
    "            x = self.block(x)\n",
    "            x = self.output(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59035ead-fcec-4ce9-9af8-44e4594db96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_channel_list: List[int]) -> None:\n",
    "        super(CAE, self).__init__()\n",
    "        n_layers = len(enc_channel_list)\n",
    "        dec_channel_list = enc_channel_list[::-1]\n",
    "        enc_layers = []\n",
    "        dec_layers = []\n",
    "        \n",
    "        for channel_idx in range(n_layers):\n",
    "            input_ch = enc_channel_list[channel_idx]\n",
    "            try:\n",
    "                next_layer_ch_idx = channel_idx + 1\n",
    "                ouput_ch = enc_channel_list[next_layer_ch_idx]\n",
    "            except IndexError:\n",
    "                break\n",
    "            conv_block = ConvBlock(\n",
    "                input_ch,\n",
    "                ouput_ch,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            )\n",
    "            enc_layers.append(conv_block)\n",
    "        \n",
    "        # TODO: Refactor input, output channel size setting\n",
    "        bottleneck_layers = [\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 4 * 32, 128),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(128, 4 * 4 * 32),\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(32, 4, 4)),\n",
    "        ]\n",
    "            \n",
    "        for channel_idx in range(n_layers):\n",
    "            input_ch = dec_channel_list[channel_idx]\n",
    "            try:\n",
    "                next_layer_ch_idx = channel_idx + 1\n",
    "                ouput_ch = dec_channel_list[next_layer_ch_idx]\n",
    "            except IndexError:\n",
    "                break\n",
    "            if channel_idx == next_layer_ch_idx - 1:\n",
    "                trans_conv_block = TransConvBlock(\n",
    "                    input_ch,\n",
    "                    ouput_ch,\n",
    "                    last_layer=True,\n",
    "                    kernel_size=2,\n",
    "                    stride=2,\n",
    "                )\n",
    "            else:\n",
    "                trans_conv_block = TransConvBlock(\n",
    "                    input_ch,\n",
    "                    ouput_ch,\n",
    "                    kernel_size=2,\n",
    "                    stride=2,\n",
    "                )\n",
    "            dec_layers.append(trans_conv_block)\n",
    "\n",
    "        layers = enc_layers + bottleneck_layers + dec_layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.net(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "545256dc",
   "metadata": {},
   "source": [
    "Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2115eeb-c9bc-4925-9fb9-41aec0f5f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channel = 1\n",
    "enc_channel_list = [input_channel, 8, 16, 32]\n",
    "lr = 1e-3\n",
    "n_epoch = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d862072e",
   "metadata": {},
   "source": [
    "Display model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1328e-a16c-4cbd-b6ff-123676ea0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(CAE(enc_channel_list).to(device), (input_channel, crop_size, crop_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(\n",
    "    CAE(enc_channel_list),\n",
    "    input_size=(batch_size, input_channel, crop_size, crop_size),\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e3287",
   "metadata": {},
   "source": [
    "### - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e585ed-0bf8-4b86-9a08-112066e97da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "verbose = True\n",
    "model = CAE(enc_channel_list).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "loss_fn = nn.MSELoss()\n",
    "loss_hist = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train_loss = 0\n",
    "    for x, _ in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        loss = loss_fn(output, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    loss_hist.append(train_loss)\n",
    "    if verbose:\n",
    "        print(f'[Epoch: {epoch}/{n_epoch}] Training Loss: {train_loss:.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "527ca6b0-c42a-4366-87aa-500e69eb6bb3",
   "metadata": {},
   "source": [
    "### - Sub pattern 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eecef4-ed32-40d1-8f58-040a4310a80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstruction_error = []\n",
    "true_labels = []\n",
    "\n",
    "model.eval()\n",
    "for X_test, y_test in tqdm(test_set):\n",
    "    X_test = X_test.to(device)\n",
    "    X_test = X_test.unsqueeze(dim=0)  # [1, 32, 32] -> [1, 1, 32, 32]\n",
    "    reconstruction = model(X_test)\n",
    "    X_test = X_test.to('cpu').detach().numpy()\n",
    "    reconstruction = reconstruction.to('cpu').detach().numpy()\n",
    "    mse = mean_squared_error(reconstruction[0][0], X_test[0][0])\n",
    "    \n",
    "    reconstruction_error.append(mse)\n",
    "    true_labels.append(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3383168a-d67d-4195-8229-c9830e260ee6",
   "metadata": {},
   "source": [
    "Sub pattern과 SEM Image No 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f630b-5d51-417c-b3ae-a737784d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = [x.split('/')[-1] for x in test_set.image_path]\n",
    "sem_no_list = [x.split('_')[0] for x in filename_list]\n",
    "container = {\n",
    "    're': reconstruction_error,\n",
    "    'label': true_labels,\n",
    "    'image_path': test_set.image_path,\n",
    "    'filename': filename_list,\n",
    "    'sem_no': sem_no_list,\n",
    "}\n",
    "result_table = pd.DataFrame(container)\n",
    "normal = result_table.query('label == 0')\n",
    "abnormal = result_table.query('label == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64106ad0-4b2b-49f2-916b-ce035fbd52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "_, bins,_ = plt.hist(normal['re'], bins=100, color='grey', label='Normal')\n",
    "plt.hist(abnormal['re'], bins=100, color='red', alpha=0.5, label='Abnormal')\n",
    "plt.legend()\n",
    "threshold = max(bins)\n",
    "plt.axvline(threshold, color='grey', linestyle=':');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fca71b-8507-4ace-ad06-35d4db2ae27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = (result_table['re'] > threshold).astype(int)\n",
    "result_table['prediction'] = prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7fb2171-6a7c-4a00-bf06-d5e290de2fd1",
   "metadata": {},
   "source": [
    "### - Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae21f7-dd74-423a-b9ca-ff19988901c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test = np.random.choice(np.arange(0, len(test_set), 1))\n",
    "filename = (test_set\n",
    "            .image_path[random_test]\n",
    "            .split('/')[-1])\n",
    "\n",
    "model.eval()\n",
    "X_test, y_test = test_set[random_test]\n",
    "X_test = X_test.view((1, input_channel, crop_size, crop_size)).to(device)\n",
    "reconstruction = model(X_test)\n",
    "\n",
    "mse = loss_fn(reconstruction, X_test).item()\n",
    "prediction = result_table.query(f'filename == \"{filename}\"')['prediction'].iloc[0]\n",
    "\n",
    "print(f'{filename}')\n",
    "print(f'Reconstruction Error: {mse}')\n",
    "print('True Class:', y_test)\n",
    "print('Predicted Class:', prediction)\n",
    "\n",
    "transpose_axes = (1, 2, 0)\n",
    "transformed_input_img = np.transpose(\n",
    "    X_test.reshape(-1, crop_size, crop_size).to('cpu'),\n",
    "    transpose_axes,\n",
    ")\n",
    "recon_img = np.transpose(\n",
    "    reconstruction.reshape(-1, crop_size, crop_size).to('cpu').detach().numpy(),\n",
    "    transpose_axes,\n",
    ")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "ax1.imshow(transformed_input_img)\n",
    "ax1.set_title('Transformed Original Image')\n",
    "ax2.imshow(recon_img)\n",
    "ax2.set_title('Reconstructed Image')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74da7c86-beb8-4e72-ae41-1a02b5bc47be",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "\n",
    "- 국소패턴 예측 결과를 바탕으로 전체 Sem Image에 대해 정상/불량 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb206e4e-7841-469e-beb5-9cd91f744c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/test'\n",
    "normal_files_t = glob(f'{path}/normal/*.png')\n",
    "abnormal_files = glob(f'{path}/abnormal/*.png')\n",
    "\n",
    "normal_sem_no_t = set([i.split('/')[-1].split('_')[0] for i in normal_files_t])\n",
    "abnormal_sem_no_t = set([i.split('/')[-1].split('_')[0] for i in abnormal_files])\n",
    "\n",
    "normal_sem_no_t = list(normal_sem_no_t)\n",
    "abnormal_sem_no_t = list(abnormal_sem_no_t)\n",
    "\n",
    "test_sem_no = result_table['sem_no'].unique()\n",
    "\n",
    "true_label = []\n",
    "\n",
    "for i in test_sem_no:\n",
    "    if i in normal_sem_no_t:\n",
    "        true_label.append(0)\n",
    "    elif i in abnormal_sem_no_t:\n",
    "        true_label.append(1)\n",
    "        \n",
    "final_prediction = pd.DataFrame({\n",
    "    'test_sem_no': test_sem_no,\n",
    "    'true_label': true_label,\n",
    "})\n",
    "\n",
    "sem_no_res = result_table.groupby('sem_no')\n",
    "\n",
    "pred_list = []\n",
    "for target in test_sem_no:\n",
    "    sem_no_res_grp = sem_no_res.get_group(target)\n",
    "    true_label = sem_no_res['label'].unique()[0]\n",
    "\n",
    "    cnt = 0\n",
    "    for subpattern_pred in sem_no_res_grp['prediction']:\n",
    "        if subpattern_pred == 1:\n",
    "            pred_list.append(1)\n",
    "            break\n",
    "        else:\n",
    "            cnt += 1\n",
    "    if cnt == len(sem_no_res_grp):\n",
    "        pred_list.append(0)\n",
    "final_prediction['prediction'] = pred_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99aa57d7-234d-4eb5-b8d2-36ff252a8f67",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3349c94-bc69-4a8d-a56f-69e6d5054022",
   "metadata": {},
   "source": [
    "### - F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78409f00-55ad-419a-a56a-eebe1611c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(final_prediction['true_label'], final_prediction['prediction'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9db50b5b-016d-4daf-943f-86144e3b3ce6",
   "metadata": {},
   "source": [
    "### - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76c6be-4f0f-4306-96cf-a72561530239",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(final_prediction['true_label'], final_prediction['prediction'])\n",
    "cm_plot = ConfusionMatrixDisplay(cm)\n",
    "cm_plot.plot(cmap='Blues');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1b79acc-7626-421d-86c4-8310a2ec6336",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeac0bd-cd07-4f99-9986-8d4b8e5aa9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input(idx: int, img_path, n_row: int, n_col: int, title: Optional[str]=None) -> None:\n",
    "    img_obj = Image.open(img_path)\n",
    "    ax = fig.add_subplot(n_row, n_col, idx + 1)\n",
    "    ax.set_axis_off()\n",
    "    if title:\n",
    "        ax.set(title=title)\n",
    "    ax.imshow(img_obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db6c480f-22fe-459b-a05b-6c75317887ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### - Wrong Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365d1a7-4e39-485d-b91e-efad4b39a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_cond = final_prediction['true_label'] != final_prediction['prediction']\n",
    "wrong_pred = final_prediction[correct_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13dce1-5c6a-4382-9deb-f6ed4ae48f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = 'JPG'\n",
    "n_sem_no = len(wrong_pred)\n",
    "n_col = 4\n",
    "n_row = int(np.ceil(n_sem_no / n_col))\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i in range(len(wrong_pred)):\n",
    "    sem_no = wrong_pred.iloc[i, 0]\n",
    "    true_label = wrong_pred.iloc[i, 1]\n",
    "    pred = wrong_pred.iloc[i, 2]\n",
    "    \n",
    "    title = f'SEM No: {sem_no} | Class: {true_label} | Pred: {pred}'\n",
    "    img_path = f'image/{sem_no}.{ext}'\n",
    "    \n",
    "    plot_input(idx=i, img_path=img_path, n_row=n_row, n_col=n_col, title=title)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f3a2d-8347-4a18-97ee-675180c5c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = result_table[result_table['sem_no'] == sem_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473105a-d499-4760-aa53-4db81855e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(target[target['label'] != target['prediction']]['image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b19c5e-6377-4d66-b9c4-1917352dd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pattern = len(paths)\n",
    "n_col = 4\n",
    "n_row = int(np.ceil(n_pattern / n_col))\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, file in enumerate(paths):\n",
    "    sem_no = file.split('/')[-1].split('.')[0]\n",
    "    title = f'Image: {sem_no}'\n",
    "    plot_input(idx=i, img_path=file, n_row=n_row, n_col=n_col, title=title)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a543fc9e-3bdc-4924-8ce3-171626b5c649",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### - By Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dff0e-90f0-472e-9dcd-c1dd1c6cd135",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filepath = (result_table\n",
    "                   .query('sem_no == \"00341\"')\n",
    "                   .query('prediction == 1')\n",
    "                   .get('image_path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30200ec-2f75-4fa2-97b2-4ba44f167937",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = len(sample_filepath)\n",
    "n_col = 4\n",
    "n_row = int(np.ceil(n_sample / n_col))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, path in enumerate(sample_filepath):\n",
    "    plot_input(idx=i, img_path=path, n_row=n_row, n_col=n_col, title=path)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "choijiwoo",
   "language": "python",
   "name": "choijiwoo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
